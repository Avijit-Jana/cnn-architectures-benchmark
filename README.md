<h1 align="center">CNN Architectures Benchmark</h1>

![GitHub repo size](https://img.shields.io/github/repo-size/Avijit-Jana/cnn-architectures-benchmark?style=plastic)
![GitHub language count](https://img.shields.io/github/languages/count/Avijit-Jana/cnn-architectures-benchmark?style=plastic)
![GitHub top language](https://img.shields.io/github/languages/top/Avijit-Jana/cnn-architectures-benchmark?style=plastic)
![GitHub last commit](https://img.shields.io/github/last-commit/Avijit-Jana/cnn-architectures-benchmark?color=red&style=plastic)

<h2>Table of Contents</h2>

- [üìñ**Project Description**](#project-description)
- [üßë‚Äçüíº**Business Use Cases**](#business-use-cases)
- [üìÅ**Data Set Explanation**](#data-set-explanation)
- [üö©**Approach**](#how-to-approach-this-project)
- [**Project Evaluation Metrics**](#project-evaluation-metrics)

## üìñProject Description

The goal of this project is to compare the performance of different CNN architectures on
various datasets. Specifically, we will evaluate LeNet-5, AlexNet, GoogLeNet, VGGNet,
ResNet, Xception, and SENet on MNIST, FMNIST, and CIFAR-10 datasets. The comparison
will be based on metrics such as loss curves, accuracy, precision, recall, and F1-score.
Comparison of CNN architectures (LeNet-5, AlexNet, GoogLeNet, VGGNet, ResNet, Xception, SENet) on MNIST, FMNIST, and CIFAR-10 datasets. Evaluates performance using loss curves, accuracy, precision, recall, and F1-score. Implemented with TensorFlow and PyTorch.

## üßë‚ÄçüíºBusiness Use Cases

The insights from this project can be applied in various business scenarios, including:

- Choosing the appropriate CNN architecture for specific computer vision tasks.
- Improving model performance by understanding the impact of dataset characteristics.
- Optimizing resource allocation by selecting models that offer the best trade-off between performance and computational cost.
- Identifying potential trade-offs between model complexity and performance.
- Understanding the impact of dataset characteristics on model performance.

## üìÅData Set Explanation

**The datasets used in this project are:**

- **MNIST**: Handwritten digits dataset consisting of 60,000 training images and 10,000 testing images. Each image is 28x28 pixels in grayscale.
- **FMNIST**: Fashion MNIST dataset consisting of 60,000 training images and 10,000 testing images of fashion products. Each image is 28x28 pixels in grayscale.
- **CIFAR-10**: Dataset consisting of 60,000 32x32 color images in 10 classes, with 50,000 training images and 10,000 testing images.

**The datasets are chosen to cover a variety of image classification tasks:**

- **MNIST and FMNIST** provide simpler tasks with grayscale images, allowing for the evaluation of basic image recognition capabilities.
- **CIFAR-10** offers a more complex task with color images, testing the models ‚Äôabilities to handle more detailed and varied data.

## How to Approach this Project

## Project Evaluation Metrics

The success and effectiveness of the project will be evaluated using the following metrics: - Accuracy: The proportion of correct predictions out of the total predictions made. - Precision: The proportion of true positive predictions out of all positive predictions made. - Recall: The proportion of true positive predictions out of all actual positives. - F1-score: The harmonic mean of precision and recall. - Loss: The value of the loss function during training and testing. 
<h3 align="middle">Developed By - Avijit Jana</h3>